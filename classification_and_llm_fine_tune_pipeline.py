# -*- coding: utf-8 -*-
"""Classification and LLM fine tune pipeline.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_lBTYY4LpXQfGBfuFnZHVT-dZnj2La8c

#Use Case 1: Classification Pipeline

##1. Prompt & Schema Design
"""

"""
You are an AI assistant trained to classify customer feedback into two categories: "Complaint" or "Praise".

Instructions:
1. Read the feedback carefully.
2. Based on the sentiment and content, classify the feedback as either "Complaint" or "Praise".
3. Return the result in the following JSON format:

{
  "label": "Complaint" | "Praise",
  "confidence": float  // value between 0 and 1 indicating confidence in the classification
}

Feedback: "<<INSERT_FEEDBACK_HERE>>"

"""

# Note :  Designed a prompt that takes raw feedback and returns a label ("Complaint" or "Praise") and confidence score.

"""##2. Evaluation

### Here are 30 simulated customer feedback entries with manual labels:

| Feedback                                                      | Ground Truth Label |
| ------------------------------------------------------------- | ------------------ |
| The delivery was late and the package was damaged.            | Complaint          |
| Amazing customer service, very helpful staff!                 | Praise             |
| I received the wrong item, not happy with the service.        | Complaint          |
| Great experience, I will shop again.                          | Praise             |
| My issue was ignored after several emails.                    | Complaint          |
| Excellent quality, totally worth the price.                   | Praise             |
| Very disappointed with the product.                           | Complaint          |
| Thank you for the fast shipping!                              | Praise             |
| The app keeps crashing every time I open it.                  | Complaint          |
| The representative was so polite and solved my issue quickly. | Praise             |
| The return process was complicated and slow.                  | Complaint          |
| I love your new collection!                                   | Praise             |
| I never received my order.                                    | Complaint          |
| This is my favorite brand now.                                | Praise             |
| Unacceptable customer service.                                | Complaint          |
| The packaging was beautiful.                                  | Praise             |
| No response from support for 5 days.                          | Complaint          |
| Very happy with the speed and accuracy.                       | Praise             |
| The instructions were unclear and confusing.                  | Complaint          |
| Everything was perfect.                                       | Praise             |
| It took too long to resolve my issue.                         | Complaint          |
| Fantastic experience, I’m very satisfied.                     | Praise             |
| I got a defective item.                                       | Complaint          |
| Best online experience I’ve had!                              | Praise             |
| Totally unusable product.                                     | Complaint          |
| Smooth checkout and fast delivery.                            | Praise             |
| App doesn't work as advertised.                               | Complaint          |
| Thank you for the amazing discount!                           | Praise             |
| Poor packaging caused the item to break.                      | Complaint          |
| Will definitely recommend to friends.                         | Praise             |

LLM Predictions (Example Simulation, e.g., using GPT-4)

 | Predicted Label | True Label | Outcome        |
| --------------- | ---------- | -------------- |
| Complaint       | Complaint  | True Positive  |
| Praise          | Praise     | True Positive  |
| Complaint       | Complaint  | True Positive  |
| Praise          | Praise     | True Positive  |
| Complaint       | Complaint  | True Positive  |
| Praise          | Praise     | True Positive  |
| Complaint       | Complaint  | True Positive  |
| Praise          | Praise     | True Positive  |
| Complaint       | Complaint  | True Positive  |
| Praise          | Praise     | True Positive  |
| Complaint       | Complaint  | True Positive  |
| Praise          | Praise     | True Positive  |
| Praise          | Complaint  | False Negative |
| Praise          | Praise     | True Positive  |
| Complaint       | Complaint  | True Positive  |
| Praise          | Praise     | True Positive  |
| Complaint       | Complaint  | True Positive  |
| Praise          | Praise     | True Positive  |
| Complaint       | Complaint  | True Positive  |
| Praise          | Praise     | True Positive  |
| Complaint       | Complaint  | True Positive  |
| Praise          | Praise     | True Positive  |
| Complaint       | Complaint  | True Positive  |
| Praise          | Praise     | True Positive  |
| Complaint       | Complaint  | True Positive  |
| Praise          | Praise     | True Positive  |
| Complaint       | Complaint  | True Positive  |
| Praise          | Praise     | True Positive  |
| Complaint       | Complaint  | True Positive  |
| Complaint       | Praise     | False Positive |

Metrics Calculation:

- True Positives (TP): 27

- False Positives (FP): 1

- False Negatives (FN): 2

- True Negatives (TN): 0 (not applicable in binary classification like this)

Formulas:

Precision = TP / (TP + FP) = 27 / (27 + 1) = 0.964

Recall = TP / (TP + FN) = 27 / (27 + 2) = 0.931

##3. Data Augmentation

LLM Prompt to Generate "Complaint" Examples:
"""

"""
You are a customer simulating real feedback to a company. Generate 100 unique
and realistic complaints about product quality, delivery issues, customer service,
app experience, or return process. Make sure each complaint is one or two sentences long,
sounds authentic, and uses casual customer language.

"""

# Note: LLM Prompt to Generate "Complaint" Examples:

"""#### Product Quality (sample 5 complaints)


- The stitching came apart after just one wash—super disappointing.

- This charger stopped working within a week of use.

- The shoes looked great online, but the sole broke in 3 days.

- Color faded after one use—what kind of dye are you using?

- I ordered a medium and it fits like an extra-small.

####Delivery Issues (sample 5 complaints)

- Package showed up 5 days late with no updates.

- The tracking link never worked.

- I received someone else’s order, not mine.

- The box was all crushed and barely taped.

- Left at the wrong house—had to track it down myself.

####Customer Service (sample 5 complaints using prompt)

- No one picks up the phone and the chat is useless.

- I've emailed three times and got no response.

- The agent kept repeating the same script, didn’t help at all.

- I was transferred five times and still no resolution.

- I waited 40 minutes on hold just to be disconnected.

####App Experience (sample 5 complaints using prompt)

- The app crashes every time I try to check out.

- It logged me out randomly and I lost my cart.

- Super slow and glitchy on Android.

- Coupons don’t apply even when they say “valid.”

- The search function never shows the right results.

####Return Process (sample 5 complaints using prompt)

- I submitted a return and never got a response.

- The return pickup was rescheduled three times.

- Your return policy is way too complicated.

- Still waiting for my refund after 2 weeks.

- They picked up the item but I haven’t heard anything since.

##4. Error Analysis





| Feedback                                                 | True Label | Predicted | Root Cause                              | Fix Proposal                                                 |
| -------------------------------------------------------- | ---------- | --------- | --------------------------------------- | ------------------------------------------------------------ |
| "Thanks for the fast refund, but the product was awful." | Complaint  | Praise    | Mixed sentiment (ambiguous language)    | Add rule to prioritize negative keywords over positive cues. |
| "Not bad, could be better."                              | Complaint  | Praise    | Vague tone, no clear emotion            | Tune LLM to detect neutral/mild dissatisfaction phrasing.    |
| "Your products are decent, but support is terrible."     | Complaint  | Praise    | Focus on product, support issue ignored | Adjust prompt to evaluate entire sentence context.           |

#Use Case 2: LLM Fine-Tuning Pipeline
"""

pip install transformers datasets evaluate rouge-score

!pip install -U transformers
import transformers
print(transformers.__version__)

# Step 1: Install required libraries (run only once)
!pip install -q transformers datasets evaluate sentencepiece

# Step 2: Import necessary packages
import os
import torch
import numpy as np
import json
from datasets import Dataset, DatasetDict
from sklearn.model_selection import train_test_split
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
from transformers import DataCollatorForSeq2Seq
from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments
import evaluate

# Step 3: Set environment variables to offline mode (optional)
os.environ["TRANSFORMERS_OFFLINE"] = "1"
os.environ["HF_DATASETS_OFFLINE"] = "1"

# Step 4: Load your JSONL data file manually
data_file_path = "/content/support_ticket_summary_50.jsonl"
records = []
with open(data_file_path, 'r') as f:
    for line in f:
        records.append(json.loads(line))

print(f"Loaded {len(records)} records")

# Step 5: Split the loaded records into train and test sets (80/20 split)
train_records, test_records = train_test_split(records, test_size=0.2, random_state=42)

# Step 6: Convert lists of dicts into Hugging Face Dataset objects
train_dataset = Dataset.from_list(train_records)
test_dataset = Dataset.from_list(test_records)

# Step 7: Create a DatasetDict for convenience
dataset = DatasetDict({
    'train': train_dataset,
    'test': test_dataset
})
print(dataset)

# Optional Step 8: Further split training set into train+validation (80/20 split)
dataset = dataset['train'].train_test_split(test_size=0.2)
print(dataset)

# Step 9: Load tokenizer and model
model_name = "google/flan-t5-base"
tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=False)
model = AutoModelForSeq2SeqLM.from_pretrained(model_name, use_auth_token=False)

# Step 10: Define max lengths for input and target sequences
max_input_length = 512
max_target_length = 128

# Step 11: Preprocessing function for tokenization
def preprocess_function(examples):
    inputs = examples["prompt"]
    targets = examples["completion"]

    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)

    # For label tokenization with T5 tokenizer
    with tokenizer.as_target_tokenizer():
        labels = tokenizer(targets, max_length=max_target_length, truncation=True)

    model_inputs["labels"] = labels["input_ids"]
    return model_inputs

# Step 12: Tokenize datasets with batching for efficiency
tokenized_datasets = dataset.map(preprocess_function, batched=True)

# Step 13: Data collator for dynamic padding during batching
data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)

# Step 14: Load ROUGE metric
rouge = evaluate.load("rouge")

# Step 15: Define compute_metrics function for trainer
def compute_metrics(eval_pred):
    predictions, labels = eval_pred

    # Decode predictions and labels to text
    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)

    # Replace -100 in labels with pad_token_id to decode properly
    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

    # Compute ROUGE scores (returns floats now)
    result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)

    # Return only rouge1 fmeasure (F1 score)
    return {"rouge1_fmeasure": result["rouge1"]}

# Step 16: Define training arguments
training_args = Seq2SeqTrainingArguments(
    output_dir="./flan-t5-support-summarizer",
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    learning_rate=5e-5,
    weight_decay=0.01,
    num_train_epochs=10,
    save_total_limit=2,
    predict_with_generate=True,
    logging_dir="./logs",
    logging_steps=20,
    save_strategy="epoch",
    push_to_hub=False,
)

# Step 17: Initialize Seq2SeqTrainer
trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["test"],
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics,
)

# Step 18: Train the model
trainer.train()

# Step 19: Save the final model and tokenizer locally
trainer.save_model("./flan-t5-support-summarizer-final")
tokenizer.save_pretrained("./flan-t5-support-summarizer-final")

# Step 20: Evaluate the model on the test set
results = trainer.evaluate(tokenized_datasets["test"])
print("Evaluation results:", results)

# Step 21: Generate predictions on the test set
preds = trainer.predict(tokenized_datasets["test"])
decoded_preds = tokenizer.batch_decode(preds.predictions, skip_special_tokens=True)
decoded_labels = tokenizer.batch_decode(
    np.where(preds.label_ids != -100, preds.label_ids, tokenizer.pad_token_id),
    skip_special_tokens=True
)

# Print some example summaries vs references
for i in range(min(3, len(decoded_preds))):
    print(f"\n=== Example {i+1} ===")
    print("Predicted summary:", decoded_preds[i])
    print("Reference summary:", decoded_labels[i])

# Step 22: Helper function to summarize new tickets
def summarize_ticket(text):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)

    inputs = tokenizer(text, return_tensors="pt", max_length=max_input_length, truncation=True).to(device)
    outputs = model.generate(**inputs, max_length=max_target_length, num_beams=4, early_stopping=True)
    summary = tokenizer.decode(outputs[0].cpu(), skip_special_tokens=True)
    return summary

# Step 23: Test summarization on a new sample ticket
sample_ticket = "User reported: Printer not working and paper jams frequently in the tray."
print("\nSummary action items:\n", summarize_ticket(sample_ticket))

"""#Fine Tunning

 - After Increasing Epoch to 20
 - Decrese Batch size to 4
 - Decrease Learning to 1e-5
"""

# Step 1: Install required libraries (run only once)
!pip install -q transformers datasets evaluate sentencepiece

# Step 2: Import necessary packages
import os
import torch
import numpy as np
import json
from datasets import Dataset, DatasetDict
from sklearn.model_selection import train_test_split
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
from transformers import DataCollatorForSeq2Seq
from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments
import evaluate

# Step 3: Set environment variables to offline mode (optional)
os.environ["TRANSFORMERS_OFFLINE"] = "1"
os.environ["HF_DATASETS_OFFLINE"] = "1"

# Step 4: Load your JSONL data file manually
data_file_path = "/content/support_ticket_summary_50.jsonl"
records = []
with open(data_file_path, 'r') as f:
    for line in f:
        records.append(json.loads(line))

print(f"Loaded {len(records)} records")

# Step 5: Split the loaded records into train and test sets (80/20 split)
train_records, test_records = train_test_split(records, test_size=0.2, random_state=42)

# Step 6: Convert lists of dicts into Hugging Face Dataset objects
train_dataset = Dataset.from_list(train_records)
test_dataset = Dataset.from_list(test_records)

# Step 7: Create a DatasetDict for convenience
dataset = DatasetDict({
    'train': train_dataset,
    'test': test_dataset
})
print(dataset)

# Optional Step 8: Further split training set into train+validation (80/20 split)
dataset = dataset['train'].train_test_split(test_size=0.2)
print(dataset)

# Step 9: Load tokenizer and model
model_name = "google/flan-t5-base"
tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=False)
model = AutoModelForSeq2SeqLM.from_pretrained(model_name, use_auth_token=False)

# Step 10: Define max lengths for input and target sequences
max_input_length = 512
max_target_length = 128

# Step 11: Preprocessing function for tokenization
def preprocess_function(examples):
    inputs = examples["prompt"]
    targets = examples["completion"]

    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)

    # For label tokenization with T5 tokenizer
    with tokenizer.as_target_tokenizer():
        labels = tokenizer(targets, max_length=max_target_length, truncation=True)

    model_inputs["labels"] = labels["input_ids"]
    return model_inputs

# Step 12: Tokenize datasets with batching for efficiency
tokenized_datasets = dataset.map(preprocess_function, batched=True)

# Step 13: Data collator for dynamic padding during batching
data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)

# Step 14: Load ROUGE metric
rouge = evaluate.load("rouge")

# Step 15: Define compute_metrics function for trainer
def compute_metrics(eval_pred):
    predictions, labels = eval_pred

    # Decode predictions and labels to text
    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)

    # Replace -100 in labels with pad_token_id to decode properly
    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

    # Compute ROUGE scores (returns floats now)
    result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)

    # Return only rouge1 fmeasure (F1 score)
    return {"rouge1_fmeasure": result["rouge1"]}

# Step 16: Define training arguments
training_args = Seq2SeqTrainingArguments(
    output_dir="./flan-t5-support-summarizer",
    per_device_train_batch_size=4,
    per_device_eval_batch_size=4,
    learning_rate=1e-5,
    weight_decay=0.01,
    num_train_epochs=20,
    save_total_limit=2,
    predict_with_generate=True,
    logging_dir="./logs",
    logging_steps=20,
    save_strategy="epoch",
    push_to_hub=False,
)

# Step 17: Initialize Seq2SeqTrainer
trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["test"],
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics,
)

# Step 18: Train the model
trainer.train()

# Step 19: Save the final model and tokenizer locally
trainer.save_model("./flan-t5-support-summarizer-final")
tokenizer.save_pretrained("./flan-t5-support-summarizer-final")

# Step 20: Evaluate the model on the test set
results = trainer.evaluate(tokenized_datasets["test"])
print("Evaluation results:", results)

# Step 21: Generate predictions on the test set
preds = trainer.predict(tokenized_datasets["test"])
decoded_preds = tokenizer.batch_decode(preds.predictions, skip_special_tokens=True)
decoded_labels = tokenizer.batch_decode(
    np.where(preds.label_ids != -100, preds.label_ids, tokenizer.pad_token_id),
    skip_special_tokens=True
)

# Print some example summaries vs references
for i in range(min(3, len(decoded_preds))):
    print(f"\n=== Example {i+1} ===")
    print("Predicted summary:", decoded_preds[i])
    print("Reference summary:", decoded_labels[i])

# Step 22: Helper function to summarize new tickets
def summarize_ticket(text):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)

    inputs = tokenizer(text, return_tensors="pt", max_length=max_input_length, truncation=True).to(device)
    outputs = model.generate(**inputs, max_length=max_target_length, num_beams=4, early_stopping=True)
    summary = tokenizer.decode(outputs[0].cpu(), skip_special_tokens=True)
    return summary

# Step 23: Test summarization on a new sample ticket
sample_ticket = "User reported: Printer not working and paper jams frequently in the tray."
print("\nSummary action items:\n", summarize_ticket(sample_ticket))

"""# Result After Small Changes

- Training Loss is lower for 20 epochs — model has learned better from training data.

- Eval Loss is significantly lower for 20 epochs, meaning better generalization.

- ROUGE-1 F1 is almost double for 20 epochs — a solid metric improvement, indicating better summary quality.

- Predictions at 20 epochs, though repetitive sometimes (like repeated "Reset password"), still include relevant keywords closer to the reference summaries.

- At 10 epochs, predictions look more generic, incomplete, or unrelated.

#Further Improvements -

- Try early stopping to avoid overfitting but still allow the model to learn well.

- Increase training data size if possible (50 records is very small).

- Experiment with different decoding methods at inference: beam search, sampling, or length penalties.

- Use a learning rate scheduler or lower learning rate for fine-tuning.

- Try data augmentation or paraphrasing to increase training examples.

- Consider better prompt engineering or task framing if model struggles to generalize.
"""

# Note - We need to use api key that can be create using link comes in result after running the model.